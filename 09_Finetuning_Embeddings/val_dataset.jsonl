{"questions": {"c4a783af-5fe8-4534-870c-3ad1861fc219": "What significant event occurred in 2024 related to the term \"slop\"?", "2cdbdbdc-28b7-43a4-9eca-1d0a1a0c3208": "Who expanded on the tweet by @deepfates regarding the term \"slop\"?", "b2c8dac9-1cdd-4452-8150-eeacf73cab82": "What is the definition of \"slop\" as it relates to AI-generated content?", "975a5699-4a9a-4ce0-87e5-91550a06ab34": "How does the term \"slop\" compare to the term \"spam\" in the context of unwanted content?", "d7902e44-1bf0-4c60-89de-ad232a6bfd87": "What does the term \"slop\" refer to in the context of generative AI usage?", "3a2bc7f0-5c1b-49d8-af8d-95cefcb68bc2": "What is \"model collapse\" and when was it first described?", "42483814-25ff-4047-bdc4-a96dc194ed60": "What is the concern regarding AI models feeding on their own output?", "72bd44fe-ca2f-42e3-b338-411724bc4cfa": "How are AI labs addressing the issue of content quality in their training processes?", "de3f467f-4f44-4662-b43e-c3f6a19c49e0": "What role does synthetic data play in the pretraining of models, particularly in the Phi series?", "87599663-672e-4a56-b7ee-818e918b53e6": "How does synthetic data compare to organic data in terms of advantages?", "3594cfb0-b9ac-40de-8be9-9d93b398083d": "How does the complexity of relationships between tokens in organic datasets affect a model's learning process?", "3a558188-9e05-447c-8a7d-d00ea72a4486": "In what way does the prediction of each token by preceding tokens facilitate reasoning for a language model?", "6faac6ea-5e7b-456f-ac1e-29d3aaa46209": "What technique is being used by an increasing number of labs to create training data for smaller models?", "ee65968b-e58b-4587-ba1b-7fd6e42b84be": "How many synthetically generated examples were used in Meta\u2019s Llama 3.3 70B fine-tuning?", "deebc062-252e-43f4-b7ea-22b8b16282e5": "What are the potential pitfalls of using LLMs as power-user tools?", "2cdb6aa0-133f-4b41-aba0-6d8a024e01cf": "How has the complexity of using LLMs changed in 2024?", "47f5ceaf-95a6-4a35-b928-e09393293b08": "What are some examples of tools that different systems can apply to problems, as mentioned in the context?", "8912d94d-ee90-40f1-b444-34a30ef8bc45": "Why is it important to understand CSP and CORS HTTP headers when building a Claude Artifact that interacts with an external API?", "286465bf-d445-42c1-b9fc-7a88260bcaeb": "What limitations do the models, including OpenAI's o1, still face despite improvements in their capabilities?", "0f583947-f111-4a38-8bf5-56ce5bbc1025": "How does the user experience with the default LLM chat UI compare to using a Linux terminal for new computer users?", "912e4f9a-c22a-45fb-a9c5-ec9e26a64e70": "What issues arise from end users developing inaccurate mental models of AI systems like ChatGPT?", "16300f0e-6300-4c26-a58a-c3384e1ebab1": "Why is it considered ludicrous to use a screenshot from ChatGPT as evidence in an argument?", "446dc373-a164-4cc3-9624-a316d9ea4ff4": "What are some reasons why better informed people have chosen to avoid using LLMs?", "d920f88f-bfd0-4e7f-be6c-b8f176aa6589": "Why is it important to develop skills for working with inherently unreliable technology like LLMs?"}, "relevant_contexts": {"c4a783af-5fe8-4534-870c-3ad1861fc219": ["0a841619-a7ed-4779-a904-0c8c422e2826"], "2cdbdbdc-28b7-43a4-9eca-1d0a1a0c3208": ["0a841619-a7ed-4779-a904-0c8c422e2826"], "b2c8dac9-1cdd-4452-8150-eeacf73cab82": ["f43a0a6b-fbc9-4cb6-8c61-763d1fbcc123"], "975a5699-4a9a-4ce0-87e5-91550a06ab34": ["f43a0a6b-fbc9-4cb6-8c61-763d1fbcc123"], "d7902e44-1bf0-4c60-89de-ad232a6bfd87": ["c08537a1-e99b-4e64-b5e0-6915b8dc2f0a"], "3a2bc7f0-5c1b-49d8-af8d-95cefcb68bc2": ["c08537a1-e99b-4e64-b5e0-6915b8dc2f0a"], "42483814-25ff-4047-bdc4-a96dc194ed60": ["9618d96c-80c7-42e8-baaf-e2f5d9ddc391"], "72bd44fe-ca2f-42e3-b338-411724bc4cfa": ["9618d96c-80c7-42e8-baaf-e2f5d9ddc391"], "de3f467f-4f44-4662-b43e-c3f6a19c49e0": ["eaa07aee-c263-40a0-ab51-93fb11e10e59"], "87599663-672e-4a56-b7ee-818e918b53e6": ["eaa07aee-c263-40a0-ab51-93fb11e10e59"], "3594cfb0-b9ac-40de-8be9-9d93b398083d": ["bb7e464f-8a5f-470c-82c2-c5070fe48abd"], "3a558188-9e05-447c-8a7d-d00ea72a4486": ["bb7e464f-8a5f-470c-82c2-c5070fe48abd"], "6faac6ea-5e7b-456f-ac1e-29d3aaa46209": ["e8686573-be35-436d-9aec-920d25b1cbba"], "ee65968b-e58b-4587-ba1b-7fd6e42b84be": ["e8686573-be35-436d-9aec-920d25b1cbba"], "deebc062-252e-43f4-b7ea-22b8b16282e5": ["f2032a0c-e88c-4388-91df-78cc957fa1e0"], "2cdb6aa0-133f-4b41-aba0-6d8a024e01cf": ["f2032a0c-e88c-4388-91df-78cc957fa1e0"], "47f5ceaf-95a6-4a35-b928-e09393293b08": ["d94cf170-546d-4dd2-9939-140031b61667"], "8912d94d-ee90-40f1-b444-34a30ef8bc45": ["d94cf170-546d-4dd2-9939-140031b61667"], "286465bf-d445-42c1-b9fc-7a88260bcaeb": ["df75b3ca-8230-4ae6-8bcf-ce96463e2731"], "0f583947-f111-4a38-8bf5-56ce5bbc1025": ["df75b3ca-8230-4ae6-8bcf-ce96463e2731"], "912e4f9a-c22a-45fb-a9c5-ec9e26a64e70": ["03c4ffd2-83aa-4f67-be58-f885795fef79"], "16300f0e-6300-4c26-a58a-c3384e1ebab1": ["03c4ffd2-83aa-4f67-be58-f885795fef79"], "446dc373-a164-4cc3-9624-a316d9ea4ff4": ["4970004b-a903-4375-9d73-2a92fc4cf5db"], "d920f88f-bfd0-4e7f-be6c-b8f176aa6589": ["4970004b-a903-4375-9d73-2a92fc4cf5db"]}, "corpus": {"0a841619-a7ed-4779-a904-0c8c422e2826": "The year of slop\n2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates:", "f43a0a6b-fbc9-4cb6-8c61-763d1fbcc123": "Watching in real time as \u201cslop\u201d becomes a term of art. the way that \u201cspam\u201d became the term for unwanted emails, \u201cslop\u201d is going in the dictionary as the term for unwanted AI generated content\n\nI expanded that definition a tiny bit to this:\n\nSlop describes AI-generated content that is both unrequested and unreviewed.\n\nI ended up getting quoted talking about slop in both the Guardian and the NY Times. Here\u2019s what I said in the NY TImes:\n\nSociety needs concise ways to talk about modern A.I. \u2014 both the positives and the negatives. \u2018Ignore that email, it\u2019s spam,\u2019 and \u2018Ignore that article, it\u2019s slop,\u2019 are both useful lessons.", "c08537a1-e99b-4e64-b5e0-6915b8dc2f0a": "I love the term \u201cslop\u201d because it so succinctly captures one of the ways we should not be using generative AI!\nSlop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot.\nSynthetic training data works great\nAn idea that surprisingly seems to have stuck in the public consciousness is that of \u201cmodel collapse\u201d. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data.", "9618d96c-80c7-42e8-baaf-e2f5d9ddc391": "The idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise!\nThat\u2019s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content\u2014deliberately creating artificial data to help steer their models in the right way.\nOne of the best descriptions I\u2019ve seen of this comes from the Phi-4 technical report, which included this:", "eaa07aee-c263-40a0-ab51-93fb11e10e59": "Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data.", "bb7e464f-8a5f-470c-82c2-c5070fe48abd": "Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns.", "e8686573-be35-436d-9aec-920d25b1cbba": "Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives\u2014a trick used by an increasing number of labs. DeepSeek v3 used \u201creasoning\u201d data created by DeepSeek-R1. Meta\u2019s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples.\nCareful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone.\nLLMs somehow got even harder to use", "f2032a0c-e88c-4388-91df-78cc957fa1e0": "A drum I\u2019ve been banging for a while is that LLMs are power-user tools\u2014they\u2019re chainsaws disguised as kitchen knives. They look deceptively simple to use\u2014how hard can it be to type messages to a chatbot?\u2014but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls.\nIf anything, this problem got worse in 2024.\nWe\u2019ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it\u2019s accurately reflected in the undocumented and secret training set.", "d94cf170-546d-4dd2-9939-140031b61667": "The number of available systems has exploded. Different systems have different tools they can apply to your problems\u2014like Python and JavaScript and web search and image generation and maybe even database lookups... so you\u2019d better understand what those tools are, what they can do and how to tell if the LLM used them or not.\nDid you know ChatGPT has two entirely different ways of running Python now?\nWant to build a Claude Artifact that talks to an external API? You\u2019d better understand CSP and CORS HTTP headers first.", "df75b3ca-8230-4ae6-8bcf-ce96463e2731": "The models may have got more capable, but most of the limitations remained the same. OpenAI\u2019s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it\u2019s running in. o1 can\u2019t run web searches or use Code Interpreter, but GPT-4o can\u2014both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023).\nWhat are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out.", "03c4ffd2-83aa-4f67-be58-f885795fef79": "Meanwhile, it\u2019s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I\u2019ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT\u2014an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right.", "4970004b-a903-4375-9d73-2a92fc4cf5db": "There\u2019s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can\u2019t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire!\nThere is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads.\nKnowledge is incredibly unevenly distributed\nMost people have heard of ChatGPT by now. How many have heard of Claude?"}}