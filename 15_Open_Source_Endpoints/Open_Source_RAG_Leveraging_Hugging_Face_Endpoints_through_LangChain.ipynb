{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcW6UWldWUMp"
      },
      "source": [
        "# Open Source RAG - Leveraging Hugging Face Endpoints through LangChain\n",
        "\n",
        "In the following notebook we will dive into the world of Open Source models hosted on Hugging Face's [inference endpoints](https://ui.endpoints.huggingface.co/).\n",
        "\n",
        "The notebook will be broken into the following parts:\n",
        "\n",
        "- 🤝 Breakout Room #2:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating LangChain components powered by the endpoints\n",
        "  4. Creating a simple RAG pipeline with [LangChain v0.2.0](https://blog.langchain.dev/langchain-v02-leap-to-stability/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-spIWt2J3Quk"
      },
      "source": [
        "## Task 1: Install required libraries\n",
        "\n",
        "Now we've got to get our required libraries!\n",
        "\n",
        "We'll start with our `langchain` and `huggingface` dependencies.\n",
        "\n",
        "> You don't need to run this cell if you're running the notebook locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "EwGLnp31jXJj"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain-huggingface langchain-community faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpZTBLwK3TIz"
      },
      "source": [
        "## Task 2: Set Environment Variables\n",
        "\n",
        "We'll need to set our `HF_TOKEN` so that we can send requests to our protected API endpoint.\n",
        "\n",
        "We'll also set-up our OpenAI API key, which we'll leverage later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NspG8I0XlFTt",
        "outputId": "edbf992c-97c0-46b1-9b69-40651a5e60d1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace Write Token: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMru14VBZAtw"
      },
      "source": [
        "## Task 3: Creating LangChain components powered by the endpoints\n",
        "\n",
        "We're going to wrap our endpoints in LangChain components in order to leverage them, thanks to LCEL, as we would any other LCEL component!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGooehdzcmPb"
      },
      "source": [
        "### HuggingFaceEndpoint for LLM\n",
        "\n",
        "We can use the `HuggingFaceEndpoint` found [here](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py) to power our chain - let's look at how we would implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N7u2Tu1FsURh"
      },
      "outputs": [],
      "source": [
        "YOUR_LLM_ENDPOINT_URL = \"https://i7nqxjdmo6rfxjky.us-east-1.aws.endpoints.huggingface.cloud\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3Cz6Mrnt2ku",
        "outputId": "f23f611f-5f08-4332-a74c-5b8d8311d185"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/richardmoss/Projects/AIMakerspace/AIE5/15_Open_Source_Endpoints/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=f\"{YOUR_LLM_ENDPOINT_URL}\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.03,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fun4XrRxZK9n"
      },
      "source": [
        "Now we can use our endpoint like we would any other LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "OFAbFT91Z8QV",
        "outputId": "588714ad-da28-4330-801b-7121b6f17ccf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/richardmoss/Projects/AIMakerspace/AIE5/15_Open_Source_Endpoints/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\" I am doing well, thanks for asking. I have been busy with work and other things, but I always make time for my blog. I have been thinking about what to write about next, and I have a few ideas. I was thinking about writing about my favorite books, or maybe about my favorite TV shows. I also thought about writing about my favorite foods, or maybe about my favorite travel destinations. What do you think? Do any of these topics sound interesting to you?\\nI would love to hear your thoughts and suggestions. Do you have any favorite books, TV shows, foods, or travel destinations that you would like to share with me? I am always looking for new ideas and inspiration, and I appreciate your input.\\nIn the meantime, I will be back with more updates and posts soon. Thank you for stopping by and reading my blog. I hope you have a great day!\\nI'm glad you're back! I've missed your posts. I think it's great that you're considering writing about your favorite books, TV shows, foods, and travel destinations. Those are all great topics that can spark interesting conversations and connections with your readers.\\nIf I had to suggest one of those topics, I'd say start with your favorite books. I'm sure many of your readers would love to know what books you enjoy reading and why. You could also share some book recommendations or discuss your favorite authors and genres.\\nAs for your other ideas, writing about your favorite TV shows could be a fun way to discuss popular culture and share your opinions on current shows. You could also write about your favorite foods and share some recipes or cooking tips. And, of course, writing about your favorite travel destinations could be a great way to inspire your readers to plan their own trips.\\nWhatever topic you choose, I'm sure your readers will appreciate your honesty and enthusiasm. You have a great blog, and I'm looking forward to seeing what you come up with next!\\nThank you for your kind words and suggestions! I'm glad you're excited about my potential topics. I think I'll start with my favorite books, as you suggested. I have a few favorite authors and series that I just can't get enough of, and I'd love to share them with my readers.\\nI'll definitely write about my favorite books, including some of my all-time favorites and some newer releases that I've enjoyed. I might also include some book recommendations and reviews, as well as some behind-the-scenes stories about how I discovered certain books or authors.\\nI'm glad you\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngH3fhw4aQ8T"
      },
      "source": [
        "Now we can add a RAG-style prompt using Llama 3 Instruct's prompt templating!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zdvv4JmkzEtj"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "<|start_header_id|>system<|end_header_id|>\n",
        "You are a helpful assistant. You answer user questions based on provided context. If you can't answer the question with the provided context, say you don't know.<|eot_id|>\n",
        "\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "User Query:\n",
        "{query}\n",
        "\n",
        "Context:\n",
        "{context}<|eot_id|>\n",
        "\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe0Qrzn4adzh"
      },
      "source": [
        "Let's create a simple LCEL chain using our prompt template Runnable and our LLM Runnable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CE4djpxM0-Fg"
      },
      "outputs": [],
      "source": [
        "rag_chain = rag_prompt | hf_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PNwrLXqDxHDY",
        "outputId": "f6803286-1aa5-488a-eea9-8bece68da7f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/richardmoss/Projects/AIMakerspace/AIE5/15_Open_Source_Endpoints/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Carl is 40 years old.'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke({\"query\" : \"Who old is Carl?\", \"context\" : \"Carl is a sweet dude, he's 40.\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emGw4-66aBfa"
      },
      "source": [
        "### HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "Now we can leverage the `HuggingFaceInferenceAPIEmbeddings` module in LangChain to connect to our Hugging Face Inference Endpoint hosted embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "n9Q7e4Gnwe_C"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
        "\n",
        "YOUR_EMBED_MODEL_URL = \"https://x1gpgq6czklqo9s9.us-east-1.aws.endpoints.huggingface.cloud\"\n",
        "\n",
        "hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
        "    model=YOUR_EMBED_MODEL_URL,\n",
        "    task=\"feature-extraction\",\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXYRBqbBayWb"
      },
      "source": [
        "Let's build a simple cosine-similarity function to verify our endpoint is working as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lOP6LKr74RG8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(phrase_1, phrase_2):\n",
        "  vec_1 = hf_embeddings.embed_documents([phrase_1])[0]\n",
        "  vec2_2 = hf_embeddings.embed_documents([phrase_2])[0]\n",
        "  return np.dot(vec_1, vec2_2) / (norm(vec_1) * norm(vec2_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZNhxF2bVIr"
      },
      "source": [
        "Let's try a few examples below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o_cqEZ34f15",
        "outputId": "d3eb4933-8842-4278-fe48-2dc15e430b60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/richardmoss/Projects/AIMakerspace/AIE5/15_Open_Source_Endpoints/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "np.float64(0.7513666430800517)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(\"I love my fluffy dog!\", \"I adore this furry puppy!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1nsAV1n4w4a",
        "outputId": "db53d783-4c87-404f-de67-fc1d01583e68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.2464151543683509)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(\"I love my fluffy dog!\", \"Trekking across the arctic is tough work.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiz6vKMlbbP4"
      },
      "source": [
        "## Task 4: Preparing Data!\n",
        "\n",
        "We'll start by loading some data from GitHub (Paul Graham's Essays) and then move to chunking them into manageable pieces!\n",
        "\n",
        "First - let's grab the repository where the files live."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkuzZben5Eqp",
        "outputId": "eb8d39ae-fd70-4691-ddaa-1f8aa15f1c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'paul-graham-to-kindle'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 36 (delta 3), reused 31 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (36/36), 2.35 MiB | 7.13 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dbredvick/paul-graham-to-kindle.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8prMk6R0bsYd"
      },
      "source": [
        "Next - we can load them using LangChain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "K155zM7e53lt"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "document_loader = TextLoader(\"./paul-graham-to-kindle/paul_graham_essays.txt\")\n",
        "documents = document_loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wYfo6_0bwVc"
      },
      "source": [
        "Now, let's split them into 1000 character pieces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-Gx_0iL6Ikc",
        "outputId": "4cd1de4f-8a7d-4727-dc92-0ce3d321a82f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4265"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=30)\n",
        "split_documents = text_splitter.split_documents(documents)\n",
        "len(split_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5HrkDhTb4i_"
      },
      "source": [
        "Just the same as we would with OpenAI's embeddings model - we can instantiate our `FAISS` vector store with our documents and our `HuggingFaceEmbeddings` model!\n",
        "\n",
        "We'll need to take a few extra steps, though, due to a few limitations of the endpoint/FAISS.\n",
        "\n",
        "We'll start by embeddings our documents in batches of `32`.\n",
        "\n",
        "> NOTE: This process might take a while depending on the compute you assigned your embedding endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ucghQgRp6YXr"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "for i in range(0, len(split_documents), 32):\n",
        "  if i == 0:\n",
        "    vectorstore = FAISS.from_documents(split_documents[i:i+32], hf_embeddings)\n",
        "    continue\n",
        "  vectorstore.add_documents(split_documents[i:i+32])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q07ZUp6Db_AO"
      },
      "source": [
        "Next, we set up FAISS as a retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fXr-yrAq7h8V"
      },
      "outputs": [],
      "source": [
        "hf_retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYrW6FRecO7U"
      },
      "source": [
        "## Task 5: Simple LCEL RAG Chain\n",
        "\n",
        "Now we can set up our LCEL RAG chain!\n",
        "\n",
        "> NOTE: We're not returning context for this example, and only returning the text output from the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ffIzIlct8ISb"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "lcel_rag_chain = {\"context\": itemgetter(\"query\") | hf_retriever, \"query\": itemgetter(\"query\")}| rag_prompt | hf_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "HOQfkEgb8nPH",
        "outputId": "92601728-d001-43e2-e543-e714d66f4f4e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/richardmoss/Projects/AIMakerspace/AIE5/15_Open_Source_Endpoints/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "/Users/richardmoss/Projects/AIMakerspace/AIE5/15_Open_Source_Endpoints/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"The best part of Silicon Valley, according to the provided context, is the combination of factors that make it an ideal place for startups, including:\\n\\n* The proximity to Stanford University, which provides a unique environment with perfect weather, beautiful mountains, and access to a cosmopolitan city (San Francisco) just 40 minutes away.\\n* The presence of talented individuals, such as students and faculty at Stanford, who are likely to start new companies.\\n* The fact that many successful startups, such as Intel, Apple, and Google, were founded in Silicon Valley, creating a self-reinforcing ecosystem.\\n\\nHowever, it's worth noting that the author of the text also mentions some weaknesses of Silicon Valley, such as its distance from San Francisco and the lack of notable landmarks or monuments.\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lcel_rag_chain.invoke({\"query\" : \"What is the best part of Silicon Valley?\"})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
